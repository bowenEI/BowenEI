<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformer | Bowen&#39;s Academic Home</title>
    <link>https://bowenei.gitee.io/tag/transformer/</link>
      <atom:link href="https://bowenei.gitee.io/tag/transformer/index.xml" rel="self" type="application/rss+xml" />
    <description>Transformer</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 30 Nov 2021 13:37:38 +0800</lastBuildDate>
    <image>
      <url>https://bowenei.gitee.io/media/icon_huc813daf5dbf7d2b27f0daba22fe1e0fb_68056_512x512_fill_lanczos_center_3.png</url>
      <title>Transformer</title>
      <link>https://bowenei.gitee.io/tag/transformer/</link>
    </image>
    
    <item>
      <title>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
      <link>https://bowenei.gitee.io/post/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale/</link>
      <pubDate>Tue, 30 Nov 2021 13:37:38 +0800</pubDate>
      <guid>https://bowenei.gitee.io/post/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Vision Transformer (ViT)&lt;/strong&gt; 是目前计算机视觉 (CV) 领域影响力最大的一项工作，因为他挑战了自从 2012 年 AlexNet 提出以来的 CNN 模型在 CV 领域的绝对统治地位。实验表明，如果能够在足够多的数据集上做预训练，那么即使不使用 CNN 也能达到同等甚至更高的精度。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ViT&lt;/strong&gt; 不仅在 CV 领域挖了一个大坑，而且还打破了 CV 和 NLP 在模型上的壁垒，所以在多模态领域也挖了一个大坑。于是，在 2020 年 10 月本文在 arXiv 上公开以后，基于 &lt;strong&gt;ViT&lt;/strong&gt; 的工作层出不穷。毫无疑问，&lt;strong&gt;ViT&lt;/strong&gt; 标志着 Transformer 模型正式杀入 CV 界，也标志着 Transformer 模型正式成为继 MLP、CNN、RNN 之后的一种新的模型范式。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2010.11929&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;原文链接&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;特别鸣谢&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;本文结合亚马逊首席科学家&lt;a href=&#34;https://github.com/mli&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;李沐&lt;/a&gt;的&lt;a href=&#34;https://www.bilibili.com/video/BV15P4y137jb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;深度学习论文精读系列视频&lt;/a&gt;进行整理。视频的主讲人是&lt;a href=&#34;https://bryanyzhu.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;朱毅&lt;/a&gt;研究员。&lt;/p&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;作者认为，Transformer 在 NLP 领域应用广泛并且成为标准，但在 CV 领域的应用仍然有限。注意力 &lt;code&gt;attention&lt;/code&gt; 机制要么与 CNN 结合使用，要么用于替换 CNN 中的某些部分而整体结构不变。作者通过实验证明，这种对 CNN 的依赖是不必要的，直接将序列化的图像块 &lt;code&gt;patches&lt;/code&gt; 输入进 Transformer 可以取得非常好的效果。尤其是在大规模的数据集上做预训练之后，再迁移到中小型数据上能够获得和最好的 CNN 相媲美的结果。&lt;/p&gt;
&lt;p&gt;朱老师提醒大家特别注意，这里作者说的花费更少的资源训练是指 TPU v3 训练 2500 天所花费的资源！&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Self-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become the model of choice in natural language processing (NLP). The dominant approach is to pre-train on a large text corpus and then fine-tune on a smaller task-specific dataset (Devlin et al., 2019). Thanks to Transformers’ computational efficiency and scalability, it has become possible to train models of unprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the models and datasets growing, there is still no sign of saturating performance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在 NLP 领域，目前主流的方式是先在一个大规模数据集上做预训练 &lt;code&gt;pre-train&lt;/code&gt;，然后在特定领域的小数据集上做微调 &lt;code&gt;fine-tune&lt;/code&gt;。（这实际上是 BERT 这篇论文里提出来的。）Transformer 的计算具有高效性 &lt;code&gt;efficiency&lt;/code&gt; 和可扩展性 &lt;code&gt;scalability&lt;/code&gt;，并且随着模型和数据集的增长，还没有看到出现性能饱和 &lt;code&gt;saturating&lt;/code&gt; 的现象。&lt;/p&gt;


&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;Transformer 直接用于 CV 领域的困难&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;朱老师认为，在过去的工作中，Transformer 一直没能用于 CV 领域的原因是因为计算的复杂性。&lt;/p&gt;
&lt;p&gt;我们试想，如果我们偏要把图片当成序列送入 Transformer 里面训练该怎么做？我们很容易想到的是将二维的图片拉直成序列，然后就可以输入到 Transformer 里面了。然而这样的计算复杂度就达到 $O(n^2)$。如果输入图片为 224x224x3，即使不考虑通道，序列长度就高达 50176，这远远大于一句话甚至是一段话的长度。&lt;/p&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;blockquote&gt;
&lt;p&gt;In computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989; Krizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing the convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while theoretically efficient, have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-like architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al., 2020).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;目前 CNN 在 CV 仍占主导地位。既然 Transformer 在 NLP 领域又特别火，注意力机制又那么香，为什么不可以在 CV 领域使用 Transformer 呢？其实是有相关工作的，一些工作将 CNN 和 self-attention 结合使用。例如，可以将网络中间的特征图当作是 Transformer 的输入。Ramachandran 等人认为可以添加一个小窗口以降低 Transformer 的计算复杂度，Wang 等人认为可以分别在图片的两个维度（宽和高）做自注意力。&lt;/p&gt;
&lt;p&gt;作者认为，上述优化理论上都是可行的，但是都是比较特殊的自注意力操作，很难在硬件上加速，训练更大的模型。个人认为，这种优化缺乏普适性。因此，传统的残差网络依然是最好的。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modifications. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classification in supervised fashion.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;作者是被 Transformer 在 NLP 领域应用的可扩展性所启发，所以他们希望将 Transformer 直接作用于图片而尽量做少的修改。作者将图片打成了很多个 &lt;code&gt;patch&lt;/code&gt;，每一个 &lt;code&gt;patch&lt;/code&gt; 的大小是 16x16。然后我们就可以将每一个 &lt;code&gt;patch&lt;/code&gt; 当成是 NLP 领域里的单词，这也就是本文标题 An Image is Worth 16x16 Words 的含义。这里作者还补充说明了训练方式是有监督训练，因为 NLP 领域绝大多数都是无监督训练。&lt;/p&gt;
&lt;p&gt;朱老师认为，读到这里可以认为作者真的是完全把 CV 任务当成是 NLP 任务去做。朱老师觉得本文其实是换了个角度讲故事，但是总而言之本文的作者只想说明一件事——Transformer在视觉领域也能取得很好的效果。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When trained on mid-sized datasets such as ImageNet without strong regularization, these models yield modest accuracies of a few percentage points below ResNets of comparable size. This seemingly discouraging outcome may be expected: Transformers lack some of the inductive biases inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果在中等大小的数据集（ImageNet）上不加以强约束，&lt;strong&gt;ViT&lt;/strong&gt; 其实是比传统的残差网络更弱的。作者于是就解释到，看起来不太好的结果其实是可以预知的。因为 Transformer 和 CNN 相比它缺少 CNN 有的归纳偏置 &lt;code&gt;inductive biases&lt;/code&gt;，它指的是一种先验知识，或者是我们提前做好的假设。&lt;/p&gt;
&lt;p&gt;CNN 其实是有两个归纳偏置。一个是 &lt;code&gt;locality&lt;/code&gt;，因为卷积运算是一个滑动窗口一点一点在图片上做的，所以就可以假设图片中相邻的区域有相似的特征。另一个是平移同变性 &lt;code&gt;translation equivariance&lt;/code&gt;，用公式表示就是 $f(g(x)) = g(f(x))$。（这里将 $f$ 理解为卷积，$g$ 理解为平移。）&lt;/p&gt;
&lt;p&gt;正因为 CNN 有这两个归纳偏置，所以 CNN 在卷积之后只需要相对更少的数据就能够学到更多的特征，得到一个更好的模型。但是对于 Transformer 来说，它其实没有这些先验信息，所以 Transformer 里的所有参数都需要从数据里面自己学习。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;However, the picture changes if the models are trained on larger datasets (14M-300M images). We find that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;果然，上了大数据以后没有归纳偏置的 Transformer 的效果要优于 CNN。&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;按照沐神读论文的方式，先来看看结论。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We have explored the direct application of Transformers to image recognition. Unlike prior works using self-attention in computer vision, we do not introduce image-specific inductive biases into the architecture apart from the initial patch extraction step. Instead, we interpret an image as a sequence of patches and process it by a standard Transformer encoder as used in NLP. This simple, yet scalable, strategy works surprisingly well when coupled with pre-training on large datasets. Thus, Vision Transformer matches or exceeds the state of the art on many image classification datasets, whilst being relatively cheap to pre-train.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本文工作最大的特点是几乎不需要任何对 CV 领域有特别深的了解，只需要把图片当成是 NLP 领域当中的序列，即序列化的图像块，就可以用 Transformer 来做了。这种方法简单、可扩展性高，并且和大规模预训练结合起来的时候效果出奇地好。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;While these initial results are encouraging, many challenges remain. One is to apply ViT to other computer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion et al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-supervised pre-training methods. Our initial experiments show improvement from self-supervised pre-training, but there is still large gap between self-supervised and large-scale supervised pre-training. Finally, further scaling of ViT would likely lead to improved performance.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;ViT&lt;/strong&gt; 属于挖坑型论文，这篇论文其实是挖了一个新模型的坑，即如何将 Transformer 应用到 CV。因此，很自然可以想到的第一个问题是，&lt;strong&gt;ViT&lt;/strong&gt; 能否在除了图像分类任务以外的任务上也达到很好的效果？例如语义分割 &lt;code&gt;segmentation&lt;/code&gt; 和目标检测 &lt;code&gt;detection&lt;/code&gt;。事实也的确如此，短短两个月不到，目标检测领域就出来了一个新的工作 ViT-FRCNN&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;，这就已经把 &lt;strong&gt;ViT&lt;/strong&gt; 用到目标检测上了。同年 12 月，语义分割也有一篇 SETR&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;
&lt;p&gt;朱老师在这里不得不吐槽一波，大家的手速实在是太快了，CV 圈卷的程度已经不能用月来计算了！而且紧接着三个月后，Swin Transformer&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; 横空出世，把多尺度设计融合到 Transformer 里面，真正让 Transformer 更适合来做 CV。&lt;/p&gt;
&lt;p&gt;另外一个可以探索的方向是自监督的训练方式，因为在 NLP 领域几乎所有基于 Transformer 的模型全都采用自监督的方式训练。本文也做了一些自监督训练的实验，但发现和有监督的训练比起来效果有明显差距。&lt;/p&gt;
&lt;p&gt;毕竟作者是 Google Brain，反正也没有谁有足够的计算资源能够填本文挖的大坑，那就自己来填吧！结果半年以后作者又提出了 Scaling Vision Transformer (ViT-G)&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;，其实就是更大的 &lt;strong&gt;ViT&lt;/strong&gt;，然后就把 ImageNet 数据集的准确率刷到 $90\%$ 以上了。&lt;/p&gt;
&lt;p&gt;这篇论文不光是挖了一个 CV 大坑，更是待到 CV 和 NLP 大一统之后，挖了一个多模态的大坑。多模态深度学习领域的工作最近也呈井喷式增长，由此可见 &lt;strong&gt;ViT&lt;/strong&gt; 这篇论文的影响力是多么巨大。&lt;/p&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Transformers were proposed by Vaswani et al. (2017) for machine translation, and have since become the state of the art method in many NLP tasks. Large Transformer-based models are often pre-trained on large corpora and then fine-tuned for the task at hand: BERT (Devlin et al., 2019) uses a denoising self-supervised pre-training task, while the GPT line of workuses language modeling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Transformer 模型目前一般都是先在一个大规模语料库上做预训练，然后在目标任务上做一些细小的微调。这里面有两大著名的工作：BERT&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; 和 GPT&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;。BERT 用的是一个被称为 &lt;code&gt;denoising&lt;/code&gt; 的自监督方式，其实就是完形填空。而 GPT 则使用 &lt;code&gt;language modeling&lt;/code&gt; 做自监督，它是指已经有一个句子，预测下一个词是什么。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Naive application of self-attention to images would require that each pixel attends to every other pixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus, to apply Transformers in the context of image processing, several approximations have been tried in the past.&lt;/p&gt;
&lt;p&gt;Many of these specialized attention architectures demonstrate promising results on computer vision tasks, but require complex engineering to be implemented efficiently on hardware accelerators.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;将自注意力简单地应用到图像的每个像素上会导致很大的计算开销，所以自注意力很难直接用到 CV。因此，想要用自注意力来处理图像就必须做一些近似 &lt;code&gt;approximation&lt;/code&gt;。下面作者列举了很多自注意力在 CV 领域的应用。例如只对邻近的像素做自注意力，或者只对一些稀疏的点做自注意力。但这些工作从本质上讲都是减少处理的数据大小，以求近似。许多这些专门的注意力架构在计算机视觉任务上展示了有希望的结果，但需要复杂的工程才能在硬件加速器上有效实施。&lt;/p&gt;
&lt;p&gt;本文这么简单的 idea 难道就没有人想到吗？其实是有类似的，作者在相关工作里面这样描述：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Most related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 ×2 from the input image and applies full self-attention on top. This model is very similar to ViT, but our work goes further to demonstrate that large scale pre-training makes vanilla transformers competitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020) use a small patch size of 2×2 pixels, which makes the model applicable only to small-resolution images, while we handle medium-resolution images as well.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ICLR 2020 有一个工作是在 CIFAR-10 数据集上切 2x2 的 &lt;code&gt;patch&lt;/code&gt;，然后在上面做 self-attention。作者认为他们的工作和这项工作的区别是在大规模数据集上做预训练，不需要任何改动就能取得比目前最好的 CNN 还好的效果。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There has also been a lot of interest in combining convolutional neural networks (CNNs) with forms of self-attention, e.g. by augmenting feature maps for image classification (Bello et al., 2019) or by further processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018; Carion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classification (Wu et al., 2020), unsupervised objectdiscovery (Locatello et al., 2020), or unified text-vision tasks (Chen et al., 2020c; Lu et al., 2019; Li et al., 2019).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;还有很多将 CNN 和自注意力结合起来的工作，而且基本涵盖了 CV 领域的很多任务。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Another recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers to image pixels after reducing image resolution and color space. The model is trained in an unsupervised fashion as a generative model, and the resulting representation can then be fine-tuned or probed linearly for classification performance, achieving a maximal accuracy of 72% on ImageNet.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;一个特别新的工作是 image GPT (iGPT)。我们知道 GPT 是 NLP 领域的代表工作，iGPT 类似，它是一个生成式模型，用无监督的方式取训练的。但是这项工作的准确率最高仅仅只有 $72\%$，而本文的准确率已经达到 $88.5\%$ 了。但是另一个之后的工作 MAE&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt; 却反而让生成式模型比之前的判别式模型效果更好，随后爆火。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Our work adds to the increasing collection of papers that explore image recognition at larger scales than the standard ImageNet dataset.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;最后作者提到了比 ImageNet 还大的数据集上各个模型的效果。&lt;/p&gt;
&lt;p&gt;总结一下，本文的相关工作列举得非常彻底，基本上和本文工作相近的工作都涵盖到了。朱老师认为，在写相关工作章节时，就是要让读者知道在你的工作之前，别人做了哪些工作，你和他们的区别在哪里。这个只要写清楚了，其实是对你非常有利的，并不会因此降低论文的创新性，反而会让这个文章变得更加简单易懂。&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;In model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible. An advantage of this intentionally simple setup is that scalable NLP Transformer architectures – and their efficient implementations – can be used almost out of the box.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;作者强调，&lt;strong&gt;ViT&lt;/strong&gt; 在模型设计上是尽可能按照最原始的 Transformer 来做的。这样做的最大好处就是可以直接把 NLP 领域成功的 Transformer 架构直接拿过来使用，不需要再魔改模型了。&lt;/p&gt;
&lt;h3 id=&#34;vision-transformer-vit&#34;&gt;Vision Transformer (ViT)&lt;/h3&gt;


















&lt;figure  id=&#34;figure-figure-1-model-overview-we-split-an-image-into-fixed-size-patches-linearly-embed-each-of-them-add-position-embeddings-and-feed-the-resulting-sequence-of-vectors-to-a-standard-transformer-encoder-in-order-to-perform-classification-we-use-the-standard-approach-of-adding-an-extra-learnable-classification-token-to-the-sequence&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Figure 1: Model overview. We split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification, we use the standard approach of adding an extra learnable “classification token” to the sequence.&#34; srcset=&#34;
               /post/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale/featured_hu64a7806ed9e97cb55f87b27af1c0aac1_126427_008558f1b5b4c64ea128a71be8e1da2f.webp 400w,
               /post/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale/featured_hu64a7806ed9e97cb55f87b27af1c0aac1_126427_34ce1b785e7793a75231026b8d859947.webp 760w,
               /post/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale/featured_hu64a7806ed9e97cb55f87b27af1c0aac1_126427_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://bowenei.gitee.io/post/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale/featured_hu64a7806ed9e97cb55f87b27af1c0aac1_126427_008558f1b5b4c64ea128a71be8e1da2f.webp&#34;
               width=&#34;760&#34;
               height=&#34;392&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Figure 1: Model overview. We split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification, we use the standard approach of adding an extra learnable “classification token” to the sequence.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;朱老师认为，论文的总览图非常重要。总览图画得好，别人在不读整篇文章的情况下光看图就能够大致了解这篇文章在讲什么。&lt;strong&gt;ViT&lt;/strong&gt; 这篇文章的总览图画得非常好，以至于其他人在引用或者讲解 &lt;strong&gt;ViT&lt;/strong&gt; 的时候都是直接把图贴上去而不做任何修改。&lt;/p&gt;
&lt;p&gt;给定一张图片，首先将这张图打成了很多 &lt;code&gt;patch&lt;/code&gt;。然后他把这些 &lt;code&gt;patch&lt;/code&gt; 转化成一个序列，每个 &lt;code&gt;patch&lt;/code&gt; 会通过一个被称为线性投射层的操作得到一个特征，即图中的 &lt;code&gt;Patch + Position Embedding&lt;/code&gt;。我们知道，自注意力机制是所有的元素之间两两去做交互，所以说 attention 本身不存在顺序问题。但是对于图片来说它是一个整体，这个九宫格是有自己的顺序的。如果顺序颠倒了，就不是原来那张图片了，所以就需要 &lt;code&gt;Position Embedding&lt;/code&gt;。加上了位置编码的信息以后，每个 &lt;code&gt;token&lt;/code&gt; 既包括了原本的图像 &lt;code&gt;patch&lt;/code&gt; 信息，又包括了图像 &lt;code&gt;patch&lt;/code&gt; 所在的位置信息。&lt;/p&gt;
&lt;p&gt;接下来实际上就和 NLP 那边是完全一样了。经过 Transformer Encoder 之后，它会给我们很多输出。那么问题来了，应该用哪个输出去做最后的分类呢？这里还需要再次借鉴 BERT&lt;sup id=&#34;fnref1:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; 当中的 extra learnable [class] embedding，即特殊字符 &lt;code&gt;cls&lt;/code&gt;。在 &lt;strong&gt;ViT&lt;/strong&gt; 中也加入了一个特殊字符，用 &lt;code&gt;*&lt;/code&gt; 代替，它的位置信息永远是 &lt;code&gt;0&lt;/code&gt;。因为自注意力机制使得每个 &lt;code&gt;token&lt;/code&gt; 之间都在互相学习，用一个空的 &lt;code&gt;token&lt;/code&gt; 就可以和图片的每个 &lt;code&gt;patch&lt;/code&gt; 交互学到完整的图片信息，所以只需要根据 &lt;code&gt;cls&lt;/code&gt; 对应的输出来判断即可。MLP Head 就是一个通用的分类头了，最后再用交叉熵函数去进行模型的训练。&lt;/p&gt;
&lt;p&gt;至于这里的 Transformer Encoder 也是完全标准的，即图中右边的部分。所以说从整体结构上来看，&lt;strong&gt;ViT&lt;/strong&gt; 的结构非常简洁，它的特殊之处就在于如何将一个图片转化成一系列的 &lt;code&gt;token&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;那么下面结合下图 &lt;strong&gt;ViT&lt;/strong&gt; 的 Transformer 部分对 Transformer 模型再做一个回顾。&lt;/p&gt;


















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34;
           src=&#34;https://bowenei.gitee.io/post/an-image-is-worth-16x16-words-transformers-for-image-recognition-at-scale/ViT.svg&#34;
           loading=&#34;lazy&#34; data-zoomable class=&#34; img-light&#34; /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

&lt;p&gt;首先将输入的图片打成若干 &lt;code&gt;patch&lt;/code&gt;，这里的图片输入大小为 224x224x3，表示宽和高为 224 像素，RGB 3 通道。&lt;code&gt;patch&lt;/code&gt; 的大小为 16x16x3，因此原图被划分为 $14 \times 14 = 196$ 个 &lt;code&gt;patch&lt;/code&gt;。再加上 &lt;code&gt;cls&lt;/code&gt;，序列的总长度为 197。&lt;/p&gt;
&lt;p&gt;经过 Embedding 和位置编码之后，得到维度为 197x768 的 &lt;code&gt;tokens&lt;/code&gt;，因为每个 16x16x3 的 &lt;code&gt;patch&lt;/code&gt; 拉直之后是 $16 \times 16 \times 3 = 768$ 维。&lt;/p&gt;
&lt;p&gt;接下来进行 Self-Attention，需要映射出 3 个矩阵 Query、Key 和 Value。由于 Transformer 的多头自注意力机制，并且 &lt;strong&gt;ViT&lt;/strong&gt; 设置了 $h = 12$ 个头，那么 3 个矩阵的维度均为 197x64，因为 $768 \div 12 = 64$。最后经过拼接 &lt;code&gt;Concat&lt;/code&gt; 得到 197x768 维的 Attention 矩阵。&lt;/p&gt;
&lt;p&gt;最后经过 MLP 全连接层，一般先把维度放大 4 倍，即 197x3012，再回到原来的维度上输出。这就是 Transformer 一层 Encoder 上的计算过程。&lt;/p&gt;
&lt;h4 id=&#34;inductive-bias&#34;&gt;Inductive bias&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;We note that Vision Transformer has much less image-specific inductive bias than CNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are baked into each layer throughout the whole model. In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are global. The two-dimensional neighborhood structure is used very sparingly: in the beginning of the model by cutting the image into patches and at fine-tuning time for adjusting the position embeddings for images of different resolution (as described below). Other than that, the position embeddings at initialization time carry no information about the 2D positions of the patches and all spatial relations between the patches have to be learned from scratch.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;作者还补充了归纳偏置的一些细节。&lt;strong&gt;ViT&lt;/strong&gt; 相较于 CNN 而言要少很多这种图像特有的归纳偏置，例如 CNN 当中有平移性和模型的局部等变性（详见前文）。但是对于 &lt;strong&gt;ViT&lt;/strong&gt; 来说，MLP 是有上述的这些归纳偏置的，但是自注意力层是全局的 &lt;code&gt;global&lt;/code&gt;，即图片的 2D 信息自注意力层没怎么用。（基本上仅仅只是 Position Embedding 的时候用到了。）另外，位置编码也是随机初始化的，并没有携带任何 2D 信息，所有的 &lt;code&gt;patch&lt;/code&gt; 之间的距离信息、场景信息都得重头学。&lt;/p&gt;
&lt;p&gt;作者补充这一段的目的是为了给后面在中小数据集上 &lt;strong&gt;ViT&lt;/strong&gt; 不如 CNN 的实验结果的解释做铺垫。那么既然如此，Transformer 在全局上表现如此优秀，CNN 又在中小数据集上快速收敛，是不是可以将它们结合起来？&lt;/p&gt;
&lt;h4 id=&#34;hybrid-architecture&#34;&gt;Hybrid Architecture&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;As an alternative to raw image patches, the input sequence can be formed from feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding projection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case, the patches can have spatial size 1x1, which means that the input sequence is obtained by simply flattening the spatial dimensions of the feature map and projecting to the Transformer dimension. The classification input embedding and position embeddings are added as described above.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;其实将 Transformer 和 CNN 的想法是可行的。我们假设现在不把图片打成 &lt;code&gt;patch&lt;/code&gt; 了，而是用一个 16x16 的卷积核去对原始图片进行卷积（步长也为 16），得到的也是一个 14x14 的特征图。后续的操作和 &lt;strong&gt;ViT&lt;/strong&gt; 一样，将特征图的每个像素当成 NLP 任务送入 Transformer Encoder 中。&lt;/p&gt;
&lt;h3 id=&#34;fine-tuning-and-higher-resolution&#34;&gt;Fine-tuning and Higher Resolution&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;When feeding images of higher resolution, we keep the patch size the same, which results in a larger effective sequence length. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints), however, the pre-trained position embeddings may no longer be meaningful. We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image. Note that this resolution adjustment and patch extraction are the only points at which an inductive bias about the 2D structure of the images is manually injected into the Vision Transformer.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;微调 &lt;code&gt;fine-tuning&lt;/code&gt; 是指预训练好的 &lt;strong&gt;ViT&lt;/strong&gt; 模型（输入图片为 224x224x3）在更大尺寸的图片（例如 320x320x3）上进行“刷脸”。但是，这对于预训练好的 &lt;strong&gt;ViT&lt;/strong&gt; 有些麻烦，因为如果 &lt;code&gt;patch&lt;/code&gt; 的大小不变，而图片变大了，于是序列变长了。从理论上说，Transformer 可以处理任意长度的序列。但是，提前训练好的位置编码可能就完全没用了，因为预训练好的位置编码具有明确的物理意义。那么预训练的位置编码还是否有用呢？作者发现，再做一个 2D 的差值就可以了。但这里的差值也仅仅只是一个临时的解决方案，应该说这算是 &lt;strong&gt;ViT&lt;/strong&gt; 在微调的时候的一个局限性。&lt;/p&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    至此，已经可以基本了解 &lt;strong&gt;ViT&lt;/strong&gt; 的基本架构了。后面实验部分以及附录的有关说明，如有需要再进行补充。
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Beal J, Kim E, Tzeng E, et al. Toward transformer-based object detection[J]. arXiv preprint arXiv:2012.09958, 2020.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Zheng S, Lu J, Zhao H, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 6881-6890.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Liu Z, Lin Y, Cao Y, et al. Swin transformer: Hierarchical vision transformer using shifted windows[J]. arXiv preprint arXiv:2103.14030, 2021.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Zhai X, Kolesnikov A, Houlsby N, et al. Scaling vision transformers[J]. arXiv preprint arXiv:2106.04560, 2021.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training[J]. 2018.&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;He K, Chen X, Xie S, et al. Masked autoencoders are scalable vision learners[J]. arXiv preprint arXiv:2111.06377, 2021.&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>Attention Is All You Need</title>
      <link>https://bowenei.gitee.io/post/attention-is-all-you-need/</link>
      <pubDate>Tue, 09 Nov 2021 22:59:13 +0800</pubDate>
      <guid>https://bowenei.gitee.io/post/attention-is-all-you-need/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Transformer&lt;/strong&gt; 是目前人工智能和深度学习领域最著名的模型之一，由 Google 团队于 2017 年 6 月提出，发表在 NeuralPS（Conference on Neural Information Processing Systems）上。起初是为了解决自然语言处理（Natural Language Processing, NLP）领域中的机器翻译问题，没想到它的效果竟然超越了循环神经网络（Recurrent Neural Networks, RNN），只需要用 &lt;code&gt;encoder&lt;/code&gt; 和 &lt;code&gt;decoder&lt;/code&gt; 以及注意力 &lt;code&gt;attention&lt;/code&gt; 机制就可以达到很好的效果。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Transformer&lt;/strong&gt; 本身是专门为 NLP 领域量身定制的，但是后来人们将图像等数据编码和序列化之后同样可以放进 &lt;strong&gt;Transformer&lt;/strong&gt; 中进行训练，并且也能让模型达到和卷积神经网络（Convolutional Neural Networks, CNN）和深度神经网络（Deep Neural Networks, DNN）相比更加出其不意的效果。这才让 &lt;strong&gt;Transformer&lt;/strong&gt; 在计算机视觉领域大火了起来。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://dl.acm.org/doi/abs/10.5555/3295222.3295349&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;原文链接&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;特别鸣谢&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;本文结合亚马逊首席科学家&lt;a href=&#34;https://github.com/mli&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;李沐&lt;/a&gt;的&lt;a href=&#34;https://www.bilibili.com/video/BV1pu411o7BE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;深度学习论文精读系列视频&lt;/a&gt;进行整理。&lt;/p&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;这篇文章最具特色的就是标题 Attention Is All You Need，翻译成中文就是“你需要注意”。后来这个标题成为了一个梗，即 xxx Is All You Need。&lt;/p&gt;
&lt;p&gt;值得注意的是，这篇文章的每一位作者后面都打了 &lt;code&gt;*&lt;/code&gt; 号，这说明这几位作者的贡献是均等的，论文首先下的注释已经充分说明了这一点。通常我们会认为论文的第一作者是主要贡献者，但是这篇文章是个例外。&lt;/p&gt;


&lt;details class=&#34;toc-inpage d-print-none  &#34; open&gt;
  &lt;summary class=&#34;font-weight-bold&#34;&gt;Table of Contents&lt;/summary&gt;
  &lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#abstract&#34;&gt;Abstract&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#7-conclusion&#34;&gt;7 Conclusion&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#1-introduction&#34;&gt;1 Introduction&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#2-background&#34;&gt;2 Background&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#3-model-architecture&#34;&gt;3 Model Architecture&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#31-encoder-and-decoder-stacks&#34;&gt;3.1 Encoder and Decoder Stacks&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#32-attention&#34;&gt;3.2 Attention&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#33-position-wise-feed-forward-networks&#34;&gt;3.3 Position-wise Feed-Forward Networks&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#34-embeddings-and-softmax&#34;&gt;3.4 Embeddings and Softmax&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#35-positional-encoding&#34;&gt;3.5 Positional Encoding&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#4-why-self-attention&#34;&gt;4 Why Self-Attention&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#5-training&#34;&gt;5 Training&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#51-training-data-and-batching&#34;&gt;5.1 Training Data and Batching&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#52-hardware-and-schedule&#34;&gt;5.2 Hardware and Schedule&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#53-optimizer&#34;&gt;5.3 Optimizer&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#54-regularization&#34;&gt;5.4 Regularization&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#6-results&#34;&gt;6 Results&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#61-machine-translation&#34;&gt;6.1 Machine Translation&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#62-model-variations&#34;&gt;6.2 Model Variations&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;/details&gt;

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所谓序列转录模型 &lt;code&gt;sequence transduction models&lt;/code&gt; 是指输入为一个序列，输出也为一个序列的模型。例如在机器翻译中，输入一段中文，然后输出其对应的英文翻译。当时（作者写这篇文章的时候），主流的序列转录模型主要基于复杂的 CNN 和 RNN，一般采用 &lt;code&gt;encoder&lt;/code&gt; 和 &lt;code&gt;decoder&lt;/code&gt; 架构。作者提出了一种基于注意力机制 &lt;code&gt;attention mechanisms&lt;/code&gt; 的网络结构 &lt;strong&gt;Transformer&lt;/strong&gt;。作者做了两个机器翻译的实验，证明了他们提出的模型效果非常好。&lt;/p&gt;
&lt;h2 id=&#34;7-conclusion&#34;&gt;7 Conclusion&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.&lt;/p&gt;
&lt;p&gt;For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.&lt;/p&gt;
&lt;p&gt;We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;按照沐神读论文的习惯，摘要读完以后直接跳到结论。沐神总结的结论主要有如下几点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Transformer&lt;/strong&gt; 是当时第一个完全基于注意力的序列转录模型，它把过去常用的循环层全部换成了 &lt;code&gt;multi-headed self-attention&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformer&lt;/strong&gt; 在机器翻译的任务中比基于循环层和卷积层的架构要快很多。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformer&lt;/strong&gt; 未来可以用在文本以外的数据类型上，例如图像、音频、视频等。现在看来，作者在当时多多少少是预测到未来的研究方向的，我十分佩服！&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/tensorflow/tensor2tensor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;仓库链接&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1 Introduction&lt;/h2&gt;
&lt;p&gt;这篇文章的导言 &lt;code&gt;Introduction&lt;/code&gt; 相对来说比较短，基本上是摘要 &lt;code&gt;Abstract&lt;/code&gt; 的扩充。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Recurrent neural networks, long short-term memory&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and gated recurrent&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;当时机器翻译最常用的模型是 RNN，主要包括如下两个著名的网络模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;LSTM&lt;/strong&gt; (Long Short-Term Memory): 长短期记忆网络。它是一种时间循环神经网络，是为了解决一般的 RNN 存在的长期依赖问题而专门设计出来的，所有的 RNN 都具有一种重复神经网络模块的链式形式。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GRU&lt;/strong&gt; (Gate Recurrent Unit): 门控循环单元。是 LSTM 网络的一种效果很好的变体，它较 LSTM 网络的结构更加简单，而且效果也很好，因此也是当前非常流形的一种网络。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;后续的工作主要围绕着循环语言模型 &lt;code&gt;recurrent language models&lt;/code&gt; 和编码器/解码器 &lt;code&gt;encoder-decoder&lt;/code&gt; 架构展开。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Recurrent models typically factor computation along the symbol positions of the input and output sequences.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;RNN 的特点是序列从左向右移一步一步往前做。当前时刻 $t$ 的隐藏状态 &lt;code&gt;hidden states&lt;/code&gt; 记作 $h_t$，它由上一个隐藏状态 $h_{t-1}$ 和当前时刻 $t$ 的输入决定。这就是为什么 RNN 能够处理时序信息的原因。也正因为 RNN 的这一特点，导致 RNN 存在如下问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算难以并行，主流的多线程 GPU 只能按照时序一个一个计算。&lt;/li&gt;
&lt;li&gt;序列长度和 $h_t$ 的长度之间的矛盾。如果序列长度特别长而 $h_t$ 不够长的话，前面的信息很可能会丢掉；但如果 $h_t$ 也设计得很长的话，内存开销太大。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;针对 RNN 的这些问题，近年来的改进工作很多，但都没有从根本上解决问题。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;注意力机制并不是本文的创新点。在现有的工作中，注意力机制已经被成功地用在编码器/解码器里面了。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;作者在本文提出的 &lt;strong&gt;Transformer&lt;/strong&gt; 则与 RNN 不同，是完全依赖注意力机制的一种模型架构。作者特别强调了他们在训练时候的并行性 &lt;code&gt;parallelization&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&#34;2-background&#34;&gt;2 Background&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet and ConvS2S, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.&lt;/p&gt;
&lt;p&gt;In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;为了解决 RNN 训练的并行性问题，有很多工作考虑采用 CNN 来代替 RNN 以增加并行性，但问题是 CNN 对长序列难以建模。例如相隔很远的两个像素块，需要多层卷积才能建立起联系。不过卷积计算的好处是可以做多个输出通道，基于此作者提出了多头注意力 &lt;code&gt;Multi-Head Attention&lt;/code&gt; 机制。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;接下来就是自注意力 &lt;code&gt;Self-attention&lt;/code&gt; 机制，这其实也是 &lt;strong&gt;Transformer&lt;/strong&gt; 中很重要的一点。不过该工作并不是 &lt;strong&gt;Transformer&lt;/strong&gt; 的创新点，已经有不少相关工作了。&lt;/p&gt;
&lt;h2 id=&#34;3-model-architecture&#34;&gt;3 Model Architecture&lt;/h2&gt;
&lt;p&gt;为了解释清楚 &lt;code&gt;encoder-decoder&lt;/code&gt;，作者首先给出如下 3 个非常重要的定义：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\left(x_1, x_2, &amp;hellip;, x_n\right)$：表示一个序列。假设这个序列是一个英文句子，那么 $x_t$ 就表示第 $t$ 个单词。&lt;/li&gt;
&lt;li&gt;$\textbf{z} = \left(z_1, z_2, &amp;hellip;, z_n\right)$：编码器的输出。$z_t$ 是 $x_t$ 的一个向量表示。&lt;/li&gt;
&lt;li&gt;$\left(y_1, y_2, &amp;hellip;, y_m\right)$：编码器的输出，是一个长为 $m$ 的序列。和编码器不同的是，解码器的词是一个个生成的，这叫做自回归 &lt;code&gt;auto-regressive&lt;/code&gt;。自回归的意思是当前的输出也会作为输入参与下一轮的输出。换句话说就是，翻译的结果出来是一个个词往外蹦儿的。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这样也就弄清楚 &lt;strong&gt;Transformer&lt;/strong&gt; 的输入和输出了，后文则主要对这里面的每个块进行说明。&lt;/p&gt;


















&lt;figure  id=&#34;figure-the-transformer---model-architecture&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The Transformer - model architecture.&#34; srcset=&#34;
               /post/attention-is-all-you-need/featured_hu8b54ce84adafe40779efe33571820b6c_96849_69939f503c2e2d5c874ab43c5d89ab3f.webp 400w,
               /post/attention-is-all-you-need/featured_hu8b54ce84adafe40779efe33571820b6c_96849_7401b658ad7641a955eec6102478b257.webp 760w,
               /post/attention-is-all-you-need/featured_hu8b54ce84adafe40779efe33571820b6c_96849_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://bowenei.gitee.io/post/attention-is-all-you-need/featured_hu8b54ce84adafe40779efe33571820b6c_96849_69939f503c2e2d5c874ab43c5d89ab3f.webp&#34;
               width=&#34;536&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      The Transformer - model architecture.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 id=&#34;31-encoder-and-decoder-stacks&#34;&gt;3.1 Encoder and Decoder Stacks&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Encoder&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;layers: $N=6$&lt;/li&gt;
&lt;li&gt;sub-layers:
&lt;ul&gt;
&lt;li&gt;multi-head self-attention mechanism: 多头自注意力机制&lt;/li&gt;
&lt;li&gt;position-wise fully connected feed-forward network: 本质上就是一个 MLP（多层感知机，Multilayer Perceptron）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;output: $\textrm{LayerNorm}(x + \textrm{Sublayer}(x))$&lt;/li&gt;
&lt;li&gt;dimension: $d_{model} = 512$&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;p&gt;&lt;strong&gt;BatchNorm 和 LayerNorm 的区别&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;沐神就 &lt;code&gt;BatchNorm&lt;/code&gt; 和 &lt;code&gt;LayerNorm&lt;/code&gt; 的区别作了详细讲解。我们知道，&lt;code&gt;Norm&lt;/code&gt; 即 &lt;code&gt;Normalization&lt;/code&gt;，对数据进行归一化处理。这和概率论中对随机变量进行标准化的操作类似，即把原向量化为均值为 $0$ 方差为 $1$ 的标准化向量。&lt;/p&gt;
&lt;p&gt;$$
\begin{align}
Y = \frac{X - \mu}{\sigma}
\end{align}
$$&lt;/p&gt;
&lt;p&gt;如图所示，&lt;code&gt;BatchNorm&lt;/code&gt; 和 &lt;code&gt;LayerNorm&lt;/code&gt; 的区别一目了然。&lt;code&gt;BatchNorm&lt;/code&gt; 是在每一个特征 &lt;code&gt;feature&lt;/code&gt; 上对 &lt;code&gt;batch&lt;/code&gt; 进行归一化，而 &lt;code&gt;LayerNorm&lt;/code&gt; 是在每一个样本 &lt;code&gt;batch&lt;/code&gt; 上对 &lt;code&gt;feature&lt;/code&gt; 进行归一化。&lt;/p&gt;
&lt;figure  id=&#34;figure-difference-between-batchnorm-and-layernorm&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Difference between BatchNorm and LayerNorm.&#34; srcset=&#34;
               /post/attention-is-all-you-need/layernorm-batchnorm_huf61da6cf384951cc94fe6d875380b93b_18215_3780baf8790d7bad91ed2224590b2089.webp 400w,
               /post/attention-is-all-you-need/layernorm-batchnorm_huf61da6cf384951cc94fe6d875380b93b_18215_190a222c2ad3ad97c203eeb42329026d.webp 760w,
               /post/attention-is-all-you-need/layernorm-batchnorm_huf61da6cf384951cc94fe6d875380b93b_18215_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://bowenei.gitee.io/post/attention-is-all-you-need/layernorm-batchnorm_huf61da6cf384951cc94fe6d875380b93b_18215_3780baf8790d7bad91ed2224590b2089.webp&#34;
               width=&#34;657&#34;
               height=&#34;276&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Difference between BatchNorm and LayerNorm.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;p&gt;为什么要使用 &lt;code&gt;LayerNorm&lt;/code&gt; 呢？一个原因是样本长度可能发生变化（即 &lt;code&gt;sequence&lt;/code&gt; 的长度 $n$），如果使用 &lt;code&gt;BatchNorm&lt;/code&gt; 的话，切片的结果可能长度参差不齐，会有很多零填充。而使用 &lt;code&gt;LayerNorm&lt;/code&gt; 则不会出现这样的问题，因为是同一个样本（即同一个序列）。由于序列长度不一有零填充，计算均值和方差的时候每个样本的计算方法不一样，不能把零算进去，因为零不是有效值。&lt;/p&gt;
&lt;p&gt;还有一点原因是，假如在做预测的时候，序列特别特别长以至于训练所得的均值和方差并不好用。而使用 &lt;code&gt;LayerNorm&lt;/code&gt; 则不会出现这样的问题，因为它是每个样本独立计算的，最后也并不像 &lt;code&gt;BatchNorm&lt;/code&gt; 那样需要算出一个全局的均值和方差。因此不管序列有多长，均值和方差都是在序列本身的基础上算的。&lt;/p&gt;

  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Decoder&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;layers: $N=6$&lt;/li&gt;
&lt;li&gt;sub-layers:
&lt;ul&gt;
&lt;li&gt;multi-head self-attention mechanism: 和 &lt;code&gt;encoder&lt;/code&gt; 相同&lt;/li&gt;
&lt;li&gt;position-wise fully connected feed-forward network: 和 &lt;code&gt;encoder&lt;/code&gt; 相同&lt;/li&gt;
&lt;li&gt;masked multi-head attention: 带掩码的多头注意力机制&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;masking: 确保位置 $i$ 的预测只能依赖于小于 $i$ 位置的已知输出。因为训练时 &lt;code&gt;decoder&lt;/code&gt; 的输入是上面一些时刻在 &lt;code&gt;encoder&lt;/code&gt; 的输出，不应该看到后面时刻的输入。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;32-attention&#34;&gt;3.2 Attention&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;注意力函数是将 &lt;code&gt;query&lt;/code&gt; 和一些键值对 &lt;code&gt;key-value pairs&lt;/code&gt; 映射成一个输出 &lt;code&gt;output&lt;/code&gt; 的函数，这里面的 &lt;code&gt;query&lt;/code&gt;、&lt;code&gt;keys&lt;/code&gt;、&lt;code&gt;values&lt;/code&gt;、&lt;code&gt;output&lt;/code&gt; 都是向量 &lt;code&gt;vectors&lt;/code&gt;。具体来说，&lt;code&gt;output&lt;/code&gt; 是 &lt;code&gt;values&lt;/code&gt; 的加权，其输出维度和 &lt;code&gt;value&lt;/code&gt; 的维度是一样的。权重是通过每个 &lt;code&gt;value&lt;/code&gt; 对应的 &lt;code&gt;key&lt;/code&gt; 和 &lt;code&gt;query&lt;/code&gt; 计算相似度 &lt;code&gt;compatibility function&lt;/code&gt; 得来的。这里的相似度针对不同的注意力机制有不同的算法。&lt;/p&gt;
&lt;h4 id=&#34;321-scaled-dot-product-attention&#34;&gt;3.2.1 Scaled Dot-Product Attention&lt;/h4&gt;
&lt;p&gt;作者在本小节主要说明了 &lt;strong&gt;Transformer&lt;/strong&gt; 采用的注意力机制。作者将之命名为 &lt;code&gt;Scaled Dot-Product Attention&lt;/code&gt;，如图所示。&lt;/p&gt;


















&lt;figure  id=&#34;figure-scaled-dot-product-attention&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Scaled Dot-Product Attention.&#34; srcset=&#34;
               /post/attention-is-all-you-need/Scaled%20Dot-Product%20Attention_hu70c1dc728f23523ace7510001a2e738b_44270_c58055229fb3f7179931f9e847824dec.webp 400w,
               /post/attention-is-all-you-need/Scaled%20Dot-Product%20Attention_hu70c1dc728f23523ace7510001a2e738b_44270_e6cb6d0f00c311b886504a93a84edf8a.webp 760w,
               /post/attention-is-all-you-need/Scaled%20Dot-Product%20Attention_hu70c1dc728f23523ace7510001a2e738b_44270_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://bowenei.gitee.io/post/attention-is-all-you-need/Scaled%20Dot-Product%20Attention_hu70c1dc728f23523ace7510001a2e738b_44270_c58055229fb3f7179931f9e847824dec.webp&#34;
               width=&#34;415&#34;
               height=&#34;681&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Scaled Dot-Product Attention.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;注意力函数的计算公式如下：&lt;/p&gt;



$$
\begin{align}
\textrm{Attention}\left(Q, K, V\right) = \textrm{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{align}
$$

&lt;p&gt;$Q$ 即 &lt;code&gt;query&lt;/code&gt;，$K$ 即 &lt;code&gt;key&lt;/code&gt;，$QK^T$ 即 &lt;code&gt;query&lt;/code&gt; 和 &lt;code&gt;key&lt;/code&gt; 做内积。作者认为，两个向量的内积值越大，说明相似度越高。除以 $\sqrt{d_k}$ 则表示单位化，然后再用 softmax 得到权重。这里的道理其实就是机器学习中的余弦相似度（余弦距离）：&lt;/p&gt;



$$
\begin{align}
\textrm{similarity} = \cos{\theta} = \frac{\alpha \cdot \beta}{||\alpha|| \cdot ||\beta||}
\end{align}
$$

&lt;p&gt;注意这里 &lt;code&gt;Mask&lt;/code&gt; 的作用是为了避免 $t$ 时刻看到后面的输入。在数学上的具体实现方式是以一个绝对值非常大的负数（$-\infty$）作为指数，计算出来的幂趋向于零，这样就实现了掩盖 $t$ 时刻后面的输入的效果。&lt;/p&gt;
&lt;h4 id=&#34;322-multi-head-attention&#34;&gt;3.2.2 Multi-Head Attention&lt;/h4&gt;
&lt;p&gt;作者认为，与其计算单个的注意力函数，不如把 &lt;code&gt;query&lt;/code&gt;、&lt;code&gt;key&lt;/code&gt;、&lt;code&gt;value&lt;/code&gt; 投影到一个更低的维度上，投影 $h$ 次，然后再计算 $h$ 次注意力函数，最后每一个函数的输出合并再投影得到最终的输出。如图所示。&lt;/p&gt;


















&lt;figure  id=&#34;figure-multi-head-attention&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Multi-Head Attention.&#34; srcset=&#34;
               /post/attention-is-all-you-need/Multi-Head%20Attention_hua1dcc6ec3dc6caf1927d2a8edcfb7688_72171_b0ffdf9e71d8f398d7da638889de4d27.webp 400w,
               /post/attention-is-all-you-need/Multi-Head%20Attention_hua1dcc6ec3dc6caf1927d2a8edcfb7688_72171_5b175683df723a45c4ed255077096fd1.webp 760w,
               /post/attention-is-all-you-need/Multi-Head%20Attention_hua1dcc6ec3dc6caf1927d2a8edcfb7688_72171_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://bowenei.gitee.io/post/attention-is-all-you-need/Multi-Head%20Attention_hua1dcc6ec3dc6caf1927d2a8edcfb7688_72171_b0ffdf9e71d8f398d7da638889de4d27.webp&#34;
               width=&#34;551&#34;
               height=&#34;685&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption data-pre=&#34;Figure&amp;nbsp;&#34; data-post=&#34;:&amp;nbsp;&#34; class=&#34;numbered&#34;&gt;
      Multi-Head Attention.
    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;多头注意力函数的计算公式如下：&lt;/p&gt;



$$
\begin{align}
\textrm{MultiHead}\left(Q, K, V\right) &amp;= \textrm{Concat}\left(\textrm{head}_1, ..., \textrm{head}_h\right)W^O \\\\
\textbf{where}\quad\textrm{head}_i &amp;= \textrm{Attention}\left(QW^Q_i, KW^K_i, VW^V_i\right)
\end{align}
$$

&lt;p&gt;在本文中作者定义 $h=8$，于是 $d_k = d_v = d_{model}/h = 64$，也就是输出维度。&lt;/p&gt;
&lt;h4 id=&#34;323-applications-of-attention-in-our-model&#34;&gt;3.2.3 Applications of Attention in our Model&lt;/h4&gt;
&lt;p&gt;这一小节作者主要介绍 &lt;strong&gt;Transformer&lt;/strong&gt; 是如何使用注意力的，归结起来一共有如下 3 种情况：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In &amp;ldquo;encoder-decoder attention&amp;rdquo; layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence.&lt;/p&gt;
&lt;p&gt;The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.&lt;/p&gt;
&lt;p&gt;Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to $-\infty$) all values in the input of the softmax which correspond to illegal connections.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;encoder&lt;/code&gt; 的 &lt;code&gt;Multi-Head Attention&lt;/code&gt; 以 &lt;code&gt;key&lt;/code&gt;、&lt;code&gt;value&lt;/code&gt;、&lt;code&gt;query&lt;/code&gt; 作为输入。图中的箭头一分为三，表示同一数据复制三次，这就叫做自注意力机制。输出的维度和输入一致。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;decoder&lt;/code&gt; 的 &lt;code&gt;Masked Multi-Head Attention&lt;/code&gt; 和 &lt;code&gt;encoder&lt;/code&gt; 的 &lt;code&gt;Multi-Head Attention&lt;/code&gt; 类似，只不过需要掩盖后面的输入，前文已详述。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;decoder&lt;/code&gt; 的 &lt;code&gt;Multi-Head Attention&lt;/code&gt; 则不再像 &lt;code&gt;encoder&lt;/code&gt; 那样是自注意力了，而是 &lt;code&gt;key&lt;/code&gt; 和 &lt;code&gt;value&lt;/code&gt; 来自于编码器的输出，&lt;code&gt;query&lt;/code&gt; 来自于解码器下一个 &lt;code&gt;attention&lt;/code&gt; 的输入。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;为了更便于大家理解，沐神举了一个非常简单的机器翻译的例子：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hello world&lt;/p&gt;
&lt;p&gt;你好世界&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;显然，输入的英文序列 $n=2$，输出的中文序列 $m=4$。当 &lt;strong&gt;Transformer&lt;/strong&gt; 在计算“好”字时，把“好”字对应的向量作为 &lt;code&gt;query&lt;/code&gt; 时，计算和 &lt;code&gt;Hello&lt;/code&gt; 对应的向量的相似度会更高一些，就会赋一个较大的权重。这就是注意力机制给我们最直观的感受！&lt;/p&gt;
&lt;h3 id=&#34;33-position-wise-feed-forward-networks&#34;&gt;3.3 Position-wise Feed-Forward Networks&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.&lt;/p&gt;
&lt;/blockquote&gt;



$$
\begin{align}
\textrm{FFN}\left(x\right) = \max \left(0, xW_1 + b_1\right)W_2 + b_2
\end{align}
$$

&lt;p&gt;在注意力层之后，&lt;code&gt;encoder&lt;/code&gt; 和 &lt;code&gt;decoder&lt;/code&gt; 都会有一个前馈网络层，首先是一个全连接层，然后是 ReLU 激活函数，最后再过一个全连接层。&lt;/p&gt;
&lt;h3 id=&#34;34-embeddings-and-softmax&#34;&gt;3.4 Embeddings and Softmax&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{model}$. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;Embeddings&lt;/code&gt; 将输入的每一个词 &lt;code&gt;token&lt;/code&gt; 映射成维度为 $d_{model}$ 的向量。&lt;code&gt;Softmax&lt;/code&gt; 的作用是归一化。&lt;/p&gt;
&lt;h3 id=&#34;35-positional-encoding&#34;&gt;3.5 Positional Encoding&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add &amp;ldquo;positional encodings&amp;rdquo; to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension $d_{model}$ as the embeddings, so that the two can be summed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;为什么需要位置编码呢？因为 &lt;code&gt;attention&lt;/code&gt; 本身并没有时序信息，它只是计算了 &lt;code&gt;key&lt;/code&gt; 和 &lt;code&gt;query&lt;/code&gt; 之间的余弦距离，它与序列的时序性无关。例如我们阅读下面这句话：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;序语倒颠不响影读阅。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们会觉得有些别扭，因为语义发生变化了。但其实 &lt;code&gt;attention&lt;/code&gt; 在计算的时候根本处理不了这种情况，这个时候就需要把时序信息加进来。与 RNN 不同的是，&lt;strong&gt;Transformer&lt;/strong&gt; 在输入里面加入时序信息，而 RNN 则是以上一时刻的输出作为部分输入。&lt;/p&gt;
&lt;h2 id=&#34;4-why-self-attention&#34;&gt;4 Why Self-Attention&lt;/h2&gt;
&lt;p&gt;本节作者介绍了为什么要采用自注意力机制。沐神认为，作者并没有把这个原因讲得特别清楚。不过，我们可以首先来看看下表：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Layer Type&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Complexity per Layer&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Sequential Operations&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Maximum Path Length&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Self-Attention&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$O(n^2 \cdot d)$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$O(1)$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$O(1)$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Recurrent&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$O(n \cdot d^2)$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$O(n)$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$O(n)$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Convolutional&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$O(k \cdot n \cdot d^2)$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$O(1)$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$O(\log_k{n})$&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Self-Attention (restricted)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$O(r \cdot n \cdot d)$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$O(1)$&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;$O(n/r)$&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;作者对比了自注意力机制、RNN、CNN 和受限制的 &lt;code&gt;restricted&lt;/code&gt; 自注意力机制的三个方面：计算复杂度、顺序计算、最大路径长度。显然计算复杂度越低越好；顺序计算是指下一步计算必须等前面几步计算完成才能计算，当然越低越好，并行度越高；最大路径长度是指一个序列信息从一个数据点走到另一个数据点需要走多远，当然越短越好。&lt;/p&gt;
&lt;p&gt;这里需要额外解释的是受限制的自注意力机制。为什么相比与自注意力机制，其计算复杂度会有所降低？是因为 &lt;code&gt;query&lt;/code&gt; 只跟最近的 $r$ 个邻居做运算。但带来的问题是最大路径长度的增加。&lt;/p&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    至此，已经可以基本了解 &lt;strong&gt;Transformer&lt;/strong&gt; 的基本架构了。后面的章节是训练和模型效果，如有需要再进行补充。
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;5-training&#34;&gt;5 Training&lt;/h2&gt;
&lt;h3 id=&#34;51-training-data-and-batching&#34;&gt;5.1 Training Data and Batching&lt;/h3&gt;
&lt;h3 id=&#34;52-hardware-and-schedule&#34;&gt;5.2 Hardware and Schedule&lt;/h3&gt;
&lt;h3 id=&#34;53-optimizer&#34;&gt;5.3 Optimizer&lt;/h3&gt;
&lt;h3 id=&#34;54-regularization&#34;&gt;5.4 Regularization&lt;/h3&gt;
&lt;h2 id=&#34;6-results&#34;&gt;6 Results&lt;/h2&gt;
&lt;h3 id=&#34;61-machine-translation&#34;&gt;6.1 Machine Translation&lt;/h3&gt;
&lt;h3 id=&#34;62-model-variations&#34;&gt;6.2 Model Variations&lt;/h3&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Hochreiter S, Schmidhuber J. Long short-term memory[J]. Neural computation, 1997, 9(8): 1735-1780.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Chung J, Gulcehre C, Cho K H, et al. Empirical evaluation of gated recurrent neural networks on sequence modeling[J]. arXiv preprint arXiv:1412.3555, 2014.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</description>
    </item>
    
  </channel>
</rss>
